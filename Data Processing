#Libraries for Dataframes
import pandas as pd
import numpy as np

# NLTK Libraries and Functions
import nltk
from nltk.corpus import sentiwordnet as swn, stopwords, wordnet
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
""
#Misc.
import re
from pprint import pprint
from collections import Counter
#Import TextBlob
from textblob import TextBlob
from textblob.sentiments import NaiveBayesAnalyzer


#Importing and cleaning the dataset
def get_call_df():
    # Already ordered by calls, and order in which conversation happened
    df = pd.read_excel('/content/Gona_Call_Transcripts.xlsx')
    # Keep needed columns
    call_df = df[['Call ID', 'Speaker Number', 'Monologue Number', 'Transcript', '# of Phrases in Monologue']]
    # Sort by speaker, monologue
    call_df = call_df.sort_values(by=['Call ID', 'Speaker Number', 'Monologue Number'])

    return call_df


#Label each monologue using textblob's NaiveBayesAnalyzer
#Use a Classifying TextBlob. We will use a Naive Bayes Classifier.
#Refer to Documentation...
# "https://textblob.readthedocs.io/en/dev/classifiers.html" to get definitions of what TextBlob is, how it works.
#Each Monologue can be its own TextBlob.
def get_monologue_df(df):
    monologue_df = df[['Call ID', 'Monologue Number', 'Transcript']]
    monologue_df['label'] = ''
    label_data(monologue_df)
    monologue_df['label'] = monologue_df['label'].apply(numeric_to_categorical)

    return monologue_df

def label_data(df):
    for i, text in df.Transcript.iteritems():
        label = TextBlob(text)
        df['label'][i] = label.sentiment.polarity
    return df

# Create function that converts numerical labels -> categorical
def numeric_to_categorical(num):
  if(num < 0 and num >= -1):
    return 'neg'
  elif num == 0:
    return 'neu'
  elif(num > 0 and num <= 1):
    return 'pos'


def get_sentence_df(df):
    sent_df = df.copy()
    sent_df['Transcript'] = sent_df.Transcript.apply(sent_tokenize)
    sentences = list(sent_df['Transcript'])
    total_sentences = flatten_list(sentences)
    tokenized_sentences = [word_tokenize(sent) for sent in total_sentences]
    num_of_words = [len(sent) for sent in tokenized_sentences]
    sentences_df = pd.DataFrame(
        data={'sent': total_sentences, 'tokenized_sent': tokenized_sentences, 'num_of_words': num_of_words})
    sentences_df = sentences_df[sentences_df['num_of_words'] > 2]
    sentences_df['label'] = ''
    sentences_df = label_data(sentences_df.copy())
    sentences_df['label'] = sentences_df['label'].apply(numeric_to_categorical)

    return sentences_df

def flatten_list(_2d_list):
    flat_list = []
    # Iterate through the outer list
    for element in _2d_list:
        if type(element) is list:
            # If the element is of type list, iterate through the sublist
            for item in element:
                flat_list.append(item)
        else:
            flat_list.append(element)
    return flat_list